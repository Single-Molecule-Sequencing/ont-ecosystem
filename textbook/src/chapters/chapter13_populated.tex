%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%% Chapter 13: Basecaller Fine-Tuning and Calibration
%% Part IV: SMA-seq and Model Improvement
%% Version 6.0 - Expanded from v5.tex lines 606-660 + NEW Content
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\chapter{Basecaller Fine-Tuning and Calibration}
\label{chap:basecaller}
\label{chap:basecaller-tuning}

\begin{learningobjectives}
By the end of this chapter, you will be able to:
\begin{itemize}
\item Understand neural network architectures underlying modern basecallers
\item Distinguish fine-tuning from training from scratch and choose appropriately
\item Apply transfer learning strategies using signal databases from Chapter~\ref{chap:noisy_labels}
\item Assess and correct quality score calibration using empirical measurements
\item Validate basecaller improvements across platforms and conditions
\item Integrate fine-tuned models into the SMA-seq framework
\end{itemize}
\end{learningobjectives}

Modern basecallers employ deep neural networks trained on millions of (signal, sequence) pairs to transform raw electrical current measurements or optical pulse traces into nucleotide sequences with associated quality scores. Although these models achieve impressive accuracy on typical sequences, their performance in specific challenging contexts: homopolymers, GC extremes, and pharmacogene variants often falls short of clinical requirements. This chapter develops strategies for fine-tuning pretrained basecallers using high-quality training data from physical standards, optimizing quality score calibration, and validating improvements to ensure generalization across diverse sequence contexts and experimental conditions.

The fine-tuning approach leverages transfer learning: rather than training basecallers from scratch (requiring millions of examples and weeks of computation), we perform targeted optimization on thousands of carefully selected examples that address specific performance gaps. This strategy dramatically improves accuracy on clinically relevant sequences while preserving general performance. Combined with the signal databases from Chapter~\ref{chap:noisy_labels} and the empirical validation via SMA-seq (Chapter~\ref{chap:sma-seq}), the fine-tuning of the base caller closes the loop from performance measurement through model improvement to validation.

\section{Basecaller Architecture Overview}
\label{sec:basecaller-architecture}

Understanding basecaller internals is essential for effective fine-tuning. While specific implementations vary across platforms, common architectural patterns emerge.

\subsection{Signal-to-Sequence Transformation}

All basecallers must solve the fundamental problem of mapping continuous time-series signals to discrete nucleotide sequences. For nanopore sequencing, the input is ionic current measurements sampled at 4-5 kHz; for PacBio, it is fluorescent pulse traces at \~{}100 Hz. The output is a nucleotide sequence with per-base quality scores indicating confidence.

This transformation is challenging because:
\begin{itemize}[itemsep=2pt]
\item Signal duration per base varies (1-10 ms for nanopore, depending on motor enzyme speed)
\item Homopolymers create ambiguous signal plateaus without sharp transitions
\item Noise, baseline drift, and artifacts corrupt signals
\item The correspondence between signal and sequence is many-to-many, not one-to-one
\end{itemize}

\subsection{Neural Network Architectures}

Modern basecallers employ deep learning to learn signal-sequence mappings from data:

\textbf{Oxford Nanopore basecallers (e.g., Guppy, Dorado):}
\begin{itemize}[itemsep=2pt]
\item \textbf{Input layer:} Raw electrical current time-series, often with 5-7 neighboring time points per input vector
\item \textbf{Convolutional layers:} 1D convolutions extract local signal features at multiple time scales
\item \textbf{Recurrent layers:} Bidirectional LSTM or GRU layers capture long-range temporal dependencies
\item \textbf{CTC decoder:} Connectionist Temporal Classification layer handles variable-length alignment between signal and sequence
\item \textbf{Output layer:} Softmax over extended alphabet (typically 5 states: A, C, G, T, blank) with quality scores derived from posterior probabilities
\end{itemize}

\textbf{PacBio basecallers:}
\begin{itemize}[itemsep=2pt]
\item \textbf{Input layer:} Fluorescence pulse width and intensity measurements from four channels (A, C, G, T)
\item \textbf{Feature layers:} Extract pulse statistics, inter-pulse intervals, and channel cross-correlations
\item \textbf{Sequence decoding:} Transform pulse features to base calls using either Hidden Markov Models (older) or recurrent neural networks (newer)
\item \textbf{Consensus layer:} For circular consensus sequencing (CCS), combine multiple passes to improve accuracy
\end{itemize}

\subsection{Platform-Specific Considerations}

Fine-tuning strategies must account for platform differences:

\begin{table}[!htbp]
\centering
\caption{Platform-Specific Basecaller Characteristics}
\label{tab:platform-characteristics}
\begin{tabular}{p{3cm}p{5cm}p{4.5cm}}
\toprule
\textbf{Platform} & \textbf{Key Characteristics} & \textbf{Fine-Tuning Implications} \\
\midrule
ONT R9.4.1 & 5-mer signal context, moderate speed, mature models & Well-suited for fine-tuning; abundant training data available \\
ONT R10.4.1 & 9-mer signal context, improved accuracy, evolving models & Requires more diverse training data due to longer context \\
PacBio Sequel II & High per-base accuracy, moderate throughput & Focus on reducing systematic errors in GC-rich regions \\
PacBio Revio & Ultra-high throughput, HiFi reads & Emphasize homopolymer accuracy and calibration \\
\bottomrule
\end{tabular}
\end{table}

The choice of pre-trained model and fine-tuning hyperparameters depends on target platform. For example, ONT R10.4.1's longer context window requires training examples spanning more diverse sequence contexts compared to R9.4.1.

\subsection{Pre-Trained Model Availability}

Most vendors provide pre-trained basecaller models optimized for general-purpose sequencing:
\begin{itemize}[itemsep=2pt]
\item \textbf{ONT:} Guppy/Dorado models released every 3-6 months, trained on internally generated data
\item \textbf{PacBio:} Models packaged with SMRT Link software, updated with new chemistry releases
\item \textbf{Community models:} Open-source alternatives like Bonito (ONT) trained on public datasets
\end{itemize}

Fine-tuning should begin with the latest official pre-trained model for the target platform. Starting from scratch is rarely justified given the computational cost (thousands of GPU-hours) and data requirements (millions of training examples) of de novo basecaller training.

\section{Fine-Tuning vs. Training from Scratch}
\label{sec:fine-tuning-vs-scratch}

A fundamental question: when should we fine-tune an existing model versus training a new model from scratch? The answer depends on multiple factors.

\subsection{Transfer Learning Benefits}

Fine-tuning leverages transfer learning: the pre-trained model has already learned general features of signal-sequence mapping (e.g., how current levels relate to nucleotide identity, temporal dynamics of base transitions). Fine-tuning adapts these general features to specific sequence contexts or experimental conditions.

\textbf{Advantages of fine-tuning:}
\begin{itemize}[itemsep=2pt]
\item \textbf{Data efficiency:} Requires only thousands of training examples versus millions for scratch training
\item \textbf{Computational efficiency:} Training completes in hours versus weeks
\item \textbf{Generalization preservation:} Retains good performance on typical sequences while improving on targeted contexts
\item \textbf{Faster iteration:} Multiple fine-tuning experiments can be run in parallel to optimize hyperparameters
\end{itemize}

\textbf{When training from scratch is necessary:}
\begin{itemize}[itemsep=2pt]
\item Novel sequencing chemistry with substantially different signal characteristics
\item Systematic biases in pre-trained model that cannot be corrected via fine-tuning
\item Access to millions of high-quality training examples enabling full retraining
\item Regulatory requirements mandating complete model transparency and custom development
\end{itemize}

For most clinical genomics applications targeting improved performance on specific genes or haplotypes, fine-tuning is the appropriate choice.

\begin{conceptbox}{Transfer Learning: Why Fine-Tuning Dominates Clinical Genomics}
The efficiency gains from transfer learning are not incremental—they are \textbf{transformational}, enabling practical applications that would be impossible with scratch training:

\textbf{Quantitative Efficiency Comparison:}
\begin{itemize}
\item \textbf{Data Requirements:} Fine-tuning requires 1,000-50,000 training examples (achievable from 10-50 plasmid standards sequenced at 100× coverage). Scratch training requires millions of examples (entire human genomes, massive computational infrastructure).

\item \textbf{Time-to-Deployment:} Fine-tuning completes in 2-48 hours on a single GPU (NVIDIA RTX 3090, \$1,500). Scratch training requires weeks on multi-GPU clusters (100+ GPUs, \$100,000+ infrastructure).

\item \textbf{Iteration Speed:} Fine-tuning enables rapid experimentation—test 20 hyperparameter combinations in a weekend. Scratch training permits only 1-2 full training runs per month.
\end{itemize}

\textbf{Why This Matters for Pharmacogenomics:}
\begin{itemize}
\item \textbf{Gene-Specific Optimization:} Create custom basecallers optimized for CYP2D6 (Chapter~\ref{chap:cyp2d6}) without retraining on entire genomes—just sequence 20 plasmid standards covering CYP2D6 structural variants, fine-tune for 24 hours, deploy.

\item \textbf{Rapid Response to New Variants:} When novel pharmacogenomic variants are discovered, fine-tune existing models within days rather than waiting months for vendor updates.

\item \textbf{Economic Feasibility:} Academic labs and small clinical operations can fine-tune basecallers with modest GPU budgets. Scratch training is limited to major sequencing companies with dedicated ML teams.
\end{itemize}

\textbf{Decision Framework:}
\begin{itemize}
\item \textbf{Use fine-tuning when:} (1) Pre-trained models exist for your platform, (2) You have 1,000+ high-quality training examples, (3) Target sequences share general features with pre-training data, (4) Deployment timeline is weeks-to-months.

\item \textbf{Consider scratch training when:} (1) Entirely novel sequencing chemistry, (2) Systematic biases in all existing models, (3) Millions of verified training examples available, (4) Multi-month development timeline acceptable.
\end{itemize}

For the vast majority of clinical genomics applications—including all pharmacogenomic haplotype classification tasks in this textbook—\textbf{fine-tuning is the only practical path forward}.
\end{conceptbox}

\subsection{Layer Selection for Fine-Tuning}

Which layers of the neural network should be fine-tuned? This choice balances adaptation flexibility with overfitting risk:

\textbf{Common strategies:}
\begin{enumerate}
\item \textbf{Output layer only:} Freeze all hidden layers, fine-tune only final softmax layer. Very safe but limited adaptation capacity. Appropriate for small datasets ($<$ 1,000 examples).

\item \textbf{Top layers:} Freeze early feature extraction layers, fine-tune last 2-3 recurrent layers and output layer. Good balance for most applications with 5,000-50,000 training examples.

\item \textbf{Full network:} Fine-tune all layers with reduced learning rate for early layers. Maximum flexibility but requires large datasets ($>$ 50,000 examples) and careful regularization.
\end{enumerate}

\textbf{Guideline:} Start with strategy (2). If validation accuracy plateaus early, try strategy (3). If overfitting occurs (training loss much lower than validation loss), revert to strategy (1) with increased regularization.

\subsection{Learning Rate Selection}

Fine-tuning requires smaller learning rates than training from scratch to avoid catastrophic forgetting (overwriting general knowledge):

\begin{eqbox}{Learning rate reduction for fine-tuning}
\begin{equation}
\eta_{\text{fine-tune}} = \alpha \cdot \eta_{\text{pretrain}}
\label{eq_13_1}
\end{equation}
\end{eqbox}

Here $\alpha \in [0.01, 0.1]$ is the reduction factor that scales down the pre-training learning rate. Typical values:
\begin{itemize}[itemsep=2pt]
\item $\alpha = 0.1$ for fine-tuning top layers only
\item $\alpha = 0.01$ for full-network fine-tuning
\item $\alpha = 0.001$ for very small training sets to prevent overfitting
\end{itemize}

Learning rate schedules with gradual decay (e.g., reduce by factor 0.5 every $N$ epochs without validation improvement) further stabilize training.

\section{Training Data Preparation}
\label{sec:training-data-prep}

High-quality training data is essential for successful fine-tuning. This section describes preparation of signal databases from Chapter~\ref{chap:noisy_labels} for basecaller optimization.

\subsection{Standard Preparation}

Training data derives from physical standards constructed following Chapter~\ref{chap:plasmid-standards}:

\textbf{Key requirements:}
\begin{enumerate}[itemsep=2pt]
\item \textbf{Sequence verification:} All standards must have independently confirmed sequences (Sanger, synthesis verification, or multiple orthogonal platforms). No ambiguities or uncertainties permitted—basecallers cannot learn from ambiguous ground truth.

\item \textbf{Purity assessment:} Quantify purity via methods in Chapter~\ref{chap:purity}. Aim for $>$ 95\% template purity to minimize label noise. Lower purity introduces training noise that degrades fine-tuning effectiveness.

\item \textbf{Context diversity:} Standards must span the sequence space targeted for improvement. For pharmacogenomics, this includes:
  \begin{itemize}[itemsep=1pt]
  \item Common and rare star allele variants
  \item Flanking intronic sequences
  \item Homopolymers of various lengths (5-15 bp)
  \item GC content extremes (20-80\%)
  \item Tandem repeats and other challenging motifs
  \end{itemize}

\item \textbf{Adequate coverage:} Each sequence context should have 50-100 read examples for robust training. With typical ONT coverage at 50-100$\times$, this requires sequencing 1-2 standards per target context.
\end{enumerate}

\subsection{Signal Extraction}

Raw signals must be extracted from sequencing output files before fine-tuning:

\textbf{For Oxford Nanopore (POD5 format):}
POD5 files store raw nanopore signals as 16-bit integers in ADC (analog-to-digital converter) space. Extract and preprocess using the official POD5 Python API:

\begin{verbatim}
import pod5
import numpy as np

signals = []
sequences = []

with pod5.Reader("reads.pod5") as reader:
    for read in reader.reads():
        # Extract signal as int16 array
        signal_adc = read.signal

        # Convert ADC to picoamperes using calibration
        offset = read.calibration.offset
        scale = read.calibration.scale
        signal_pa = (signal_adc - offset) * scale

        # Normalize: mean-center and scale to unit variance
        signal_norm = (signal_pa - np.mean(signal_pa)) / np.std(signal_pa)

        # Filter by end_reason (Chapter 11)
        if read.end_reason.name == 'signal_positive':
            signals.append(signal_norm)
            # Match with basecalled sequence by read_id
            # sequences.append(basecalled_sequences[read_id])  # Replace with actual lookup from your basecalled data
\end{verbatim}

\textbf{Key POD5 metadata fields:}
\begin{itemize}[itemsep=2pt]
\item \texttt{read.signal}: Raw signal array (int16, ADC space)
\item \texttt{read.calibration.offset}: ADC-to-pA offset parameter
\item \texttt{read.calibration.scale}: ADC-to-pA scale factor
\item \texttt{read.end\_reason}: Read termination cause (Section~\ref{sec:end-reason-analysis})
\item \texttt{read.sample\_count}: Number of signal samples
\item \texttt{read.median\_before}: Median current before read (baseline)
\end{itemize}

\textbf{Signal normalization:} Mean-centering and unit variance scaling per read ensures consistent neural network input. Alternative: quantile normalization or median absolute deviation (MAD) scaling for robustness to outliers.

\textbf{Quality filtering:}
\begin{itemize}[itemsep=2pt]
\item Remove reads with excessive baseline drift (linear fit residuals $>3\sigma$)
\item Filter reads with abnormal signal variance (outside 0.5--2.0$\times$ median)
\item Exclude reads shorter than minimum length threshold (e.g., $<$500 bp)
\item Retain only \texttt{end\_reason = signal\_positive} for complete molecules
\end{itemize}

\textbf{Storage format:} Store processed signals in HDF5 (efficient random access) or compressed NumPy arrays (.npz). Include metadata: read ID, sequence, quality scores, end\_reason.

\textbf{For PacBio:}
\begin{itemize}[itemsep=2pt]
\item Extract pulse width and intensity from BAM files (requires \texttt{bam2bax} conversion)
\item For HiFi reads, store raw subreads before consensus for maximum signal information
\item Normalize pulse features within each ZMW (zero-mode waveguide)
\item Filter artifacts: remove pulses with abnormal width-intensity relationships
\end{itemize}

\subsection{Train/Validation/Test Splits}

Proper data partitioning prevents overfitting and enables reliable performance assessment:

\begin{table}[!htbp]
\centering
\caption{Recommended Data Splits for Fine-Tuning}
\label{tab:data-splits}
\begin{tabular}{p{2.5cm}p{3cm}p{2.5cm}p{4cm}}
\toprule
\textbf{Split} & \textbf{Proportion} & \textbf{Sample Size} & \textbf{Purpose} \\
\midrule
Training & 70\% & 7,000 reads & Gradient descent optimization \\
Validation & 15\% & 1,500 reads & Hyperparameter tuning, early stopping \\
Test & 15\% & 1,500 reads & Final unbiased performance assessment \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Critical considerations:}
\begin{itemize}[itemsep=2pt]
\item \textbf{Stratification:} Ensure all splits contain representatives of each sequence context. Don't accidentally place all homopolymer examples in training set, leaving none for validation.

\item \textbf{Independence:} Reads from the same DNA molecule (e.g., PCR duplicates) must be in the same split to prevent information leakage.

\item \textbf{Validation stability:} The validation set must be large enough that metrics have low variance. For binary classification, 1,500 examples yields $\pm$2\% confidence intervals.

\item \textbf{Test set sanctity:} Never use test set for any decisions during development. It exists solely for final unbiased evaluation before deployment.
\end{itemize}

\subsection{Data Augmentation}

Limited training data can be augmented to improve generalization:

\textbf{Augmentation strategies:}
\begin{itemize}[itemsep=2pt]
\item \textbf{Signal perturbation:} Add small Gaussian noise to signals ($\sigma \approx 0.05 \times$ signal standard deviation)
\item \textbf{Time warping:} Slightly stretch or compress signals in time (5-10\% variation) to simulate motor enzyme speed variation
\item \textbf{Baseline drift:} Add slow polynomial trends to simulate experimental artifacts
\item \textbf{Reverse complement:} For nanopore data, reverse signals and complement sequences (doubling effective dataset size)
\end{itemize}

\textbf{Caution:} Excessive augmentation can introduce artifacts that degrade performance. Validate augmented model performance on unaugmented test data to ensure augmentation helps rather than harms.

\section{Fine-Tuning Methodology}
\label{sec:fine-tuning-methodology}

This section presents the complete fine-tuning procedure, integrating concepts from Chapters~\ref{chap:noisy_labels} and \ref{chap:sma-seq}.

\subsection{Supervised Fine-Tuning with Ground Truth Labels}

One of SMA-seq's most powerful applications lies in providing high-quality labeled training data for supervised machine learning approaches to basecaller improvement. Modern basecallers employ deep neural networks trained on large datasets, but their performance on specific sequence contexts depends critically on the quality and diversity of training data.

SMA-seq experiments (Chapter~\ref{chap:sma-seq}) generate precisely what neural network optimization requires: pairs of raw signal data and known-truth sequence labels. Unlike typical basecaller training that relies on consensus sequences or reference alignments (which may themselves contain errors), SMA-seq provides definitive ground truth through engineered standards.

The fine-tuning process begins with a pre-trained basecaller model that has learned general signal-to-sequence mapping from large-scale training. We then perform additional gradient descent optimization using SMA-seq-derived labeled examples:

\begin{eqbox}{Supervised fine-tuning objective}
\begin{equation}
\boldsymbol{\theta}^* = \argmin_{\boldsymbol{\theta}} \sum_{(x, s) \in \mathcal{D}_{\text{SEER}}} \mathcal{L}(f_{\text{basecaller}}(x; \boldsymbol{\theta}), s) + \lambda \mathcal{R}(\boldsymbol{\theta})
\label{eq_13_2}
\end{equation}
\end{eqbox}

where $\mathcal{D}_{\text{SEER}}$ represents the set of (signal, known-sequence) pairs from SMA-seq experiments, $\mathcal{L}$ denotes the loss function (typically cross-entropy for sequence prediction), $f_{\text{basecaller}}$ represents the neural network model with parameters $\boldsymbol{\theta}$, and $\mathcal{R}(\boldsymbol{\theta})$ provides regularization to prevent overfitting (typically $L_2$ penalty on weight magnitudes).

\subsection{Targeted Sequence Selection}

Effective basecaller optimization requires strategic selection of which sequences to include in SMA-seq standards. The goal is to generate training data that maximally improves overall performance while efficiently using experimental resources. This optimization problem balances multiple considerations.

First, we should prioritize sequence contexts that appear frequently in target applications (e.g., pharmacogenes for clinical pharmacogenomics). Second, we should target contexts where current basecaller performance is suboptimal, as these offer the greatest potential for improvement. Third, we should ensure sufficient diversity to enable generalization rather than narrow overfitting.

An active learning framework naturally addresses these considerations. After each round of SMA-seq experiments and fine-tuning, we evaluate the improved basecaller on held-out validation sequences. Sequences where the model continues to underperform become candidates for inclusion in the next round of SMA-seq standards. This iterative process progressively eliminates performance gaps while efficiently focusing experimental effort where it provides maximum value.

\begin{algorithm}[H]
\caption{Active Learning for Targeted Basecaller Improvement}
\label{alg:active-learning}
\begin{algorithmic}[1]
\STATE Initialize with general pre-trained model $\theta^{(0)}$
\STATE Define target sequence set $\mathcal{S}_{\text{target}}$ (e.g., pharmacogene alleles)
\FOR{round $r = 1, 2, \ldots, R$}
  \STATE Evaluate model $\theta^{(r-1)}$ on validation set, compute per-sequence error rates
  \STATE Identify $K$ sequences with highest error rates: $\mathcal{S}_{\text{poor}}^{(r)}$
  \STATE Construct SMA-seq standards for $\mathcal{S}_{\text{poor}}^{(r)}$ (Chapter~\ref{chap:plasmid-standards})
  \STATE Sequence standards, extract signal-label pairs $\mathcal{D}^{(r)}$
  \STATE Fine-tune model: $\theta^{(r)} \gets$ optimize using $\mathcal{D}^{(r)}$ via Equation~\ref{eq_13_2}
  \STATE Validate improvement on test set
  \IF{performance satisfies clinical requirements}
    \STATE \textbf{break} (success)
  \ENDIF
\ENDFOR
\STATE \textbf{return} $\theta^{(R)}$
\end{algorithmic}
\end{algorithm}

This algorithm requires 2-4 iterations in typical applications, with each iteration adding 10-20 new standards targeting identified performance gaps. The iterative approach is far more efficient than attempting to design comprehensive standards upfront without knowledge of specific weaknesses.

\subsection{Loss Function Selection}

The choice of loss function $\mathcal{L}$ in Equation~\ref{eq_13_2} affects convergence speed and final performance:

\begin{eqbox}{Cross-entropy loss for sequence prediction}
\begin{equation}
\mathcal{L}_{\text{CE}}(\hat{s}, s) = -\sum_{i=1}^{L} \sum_{b \in \{A,C,G,T\}} \mathbb{I}\{s_i = b\} \log P(\hat{s}_i = b)
\label{eq_13_3}
\end{equation}
\end{eqbox}

This is the standard loss function appropriate for most applications. It directly optimizes the log-likelihood of the correct sequence, where $s$ is the ground truth sequence, $\hat{s}$ is the predicted sequence, $L$ is the sequence length, and $\mathbb{I}\{\cdot\}$ is the indicator function.

\begin{eqbox}{CTC loss for variable-length alignment}
\begin{equation}
\mathcal{L}_{\text{CTC}}(\hat{s}, s) = -\log \sum_{\pi \in \mathcal{A}(s)} P(\pi | x)
\label{eq_13_4}
\end{equation}
\end{eqbox}

This Connectionist Temporal Classification (CTC) loss is commonly used for nanopore basecallers with CTC decoder layers. Here $\mathcal{A}(s)$ is the set of all alignments consistent with sequence $s$, and the loss handles variable-length alignment automatically by marginalizing over all valid alignments.

\textbf{Focal Loss (For Imbalanced Data):}
When fine-tuning on rare variants:
\begin{equation}
\mathcal{L}_{\text{focal}}(\hat{s}, s) = -\sum_{i=1}^{L} (1 - P(\hat{s}_i = s_i))^\gamma \log P(\hat{s}_i = s_i)
\label{eq_13_5}
\end{equation}

with $\gamma \in [1, 3]$. The $(1-p)^\gamma$ term downweights easy examples, focusing learning on difficult cases.

\subsection{Hyperparameter Optimization}

Fine-tuning success depends on appropriate hyperparameter selection:

\begin{table}[!htbp]
\centering
\caption{Key Fine-Tuning Hyperparameters}
\label{tab:hyperparameters}
\begin{tabular}{p{3.5cm}p{3cm}p{5cm}}
\toprule
\textbf{Hyperparameter} & \textbf{Typical Range} & \textbf{Selection Guidance} \\
\midrule
Learning rate $\eta$ & $10^{-5}$ to $10^{-3}$ & Start with $10^{-4}$; reduce if training unstable \\
Batch size & 32 to 256 & Larger for more data; GPU memory limited \\
Regularization $\lambda$ & $10^{-5}$ to $10^{-3}$ & Increase if validation loss > training loss \\
Dropout rate $p$ & 0.1 to 0.5 & Increase if overfitting observed \\
Gradient clip threshold & 1.0 to 5.0 & Prevents exploding gradients in RNNs \\
Early stopping patience & 5 to 20 epochs & More patience for noisy validation metrics \\
\bottomrule
\end{tabular}
\end{table}

\textbf{Hyperparameter search strategy:}
\begin{enumerate}[itemsep=2pt]
\item Start with defaults from pre-trained model documentation
\item Conduct grid search over learning rate (most critical parameter)
\item If overfitting occurs, increase regularization and dropout
\item Use validation set to select best configuration
\item Confirm on test set that selected hyperparameters generalize
\end{enumerate}

\section{Quality Score Calibration}
\label{sec:quality-calibration}

Even after fine-tuning sequence accuracy, quality scores may remain miscalibrated. This section addresses assessment and correction of quality score calibration using SMA-seq measurements.

\subsection{Calibration Assessment}

Recall from Chapter~\ref{chap:sma-seq} the quality overestimation metric:

\begin{equation}
d = \frac{Q_{\text{emp}}(u) - \bar{Q}_{\text{pred}}(u)}{\bar{Q}_{\text{pred}}(u)}
\label{eq_13_6}
\end{equation}

where $Q_{\text{emp}}$ is the empirical quality (measured from edit distance to known sequence) and $\bar{Q}_{\text{pred}}$ is the mean predicted quality score. The acceptance criterion from \CEref{14} requires $|d| \leq 0.30$ for clinical applications.

\textbf{ONT-Specific Calibration Considerations:}
ONT quality scores (Phred+33, Q0--Q93 range) are generated by neural network confidence estimates. Research indicates systematic overestimation in the Q7--Q30 range, where most reads cluster. This occurs because:
\begin{itemize}[itemsep=1pt]
\item Neural networks trained on general genomic data may not generalize to pharmacogene-specific sequence contexts
\item Homopolymer regions compress quality score distributions toward lower values
\item GC-rich regions show different calibration characteristics than AT-rich regions
\end{itemize}

\textbf{Calibration diagnostics:}

\textbf{1. QQ-plots:} Plot predicted vs. empirical error rates at various quality score thresholds:
\begin{itemize}[itemsep=1pt]
\item Perfect calibration: points fall on identity line
\item Points above line: underconfident (quality scores too conservative)
\item Points below line: overconfident (quality scores too optimistic)
\end{itemize}

\begin{eqbox}{Expected calibration error (ECE)}
\begin{equation}
\text{ECE} = \sum_{i=1}^{B} \frac{n_i}{N} |\text{acc}(Q_i) - \bar{Q}_i|
\label{eq_13_7}
\end{equation}
\end{eqbox}

This Expected Calibration Error (ECE) quantifies calibration quality across the quality score range. Here $B$ bins partition the quality score range, $n_i$ is the number of bases in bin $i$, $\text{acc}(Q_i)$ is the empirical accuracy in bin $i$, and $\bar{Q}_i$ is the mean predicted quality in that bin. Well-calibrated models have ECE $< 0.05$.

\subsection{Recalibration Methods}

When systematic miscalibration is detected ($d > 0.30$ or ECE $> 0.05$), recalibration transforms predicted quality scores to correct empirical scores:

\textbf{Linear recalibration:}
\begin{equation}
Q_{\text{cal}} = \alpha + \beta Q_{\text{pred}}
\label{eq_13_8}
\end{equation}

Estimate $\alpha$, $\beta$ via linear regression of empirical quality on predicted quality using SMA-seq validation data. Simple and effective for uniform shifts or scaling.

\textbf{Isotonic regression:}
\begin{equation}
Q_{\text{cal}} = f_{\text{iso}}(Q_{\text{pred}})
\label{eq_13_9}
\end{equation}

where $f_{\text{iso}}$ is a monotonically increasing function fit to minimize squared error between predicted and empirical scores. More flexible than linear recalibration; can correct non-linear miscalibration patterns.

\textbf{Bin-based lookup:}
Partition quality score range into bins, compute empirical accuracy in each bin, assign calibrated quality as bin mean. Nonparametric and robust, but requires large validation sets for stable estimates.

\subsection{Recalibration Validation}

Recalibration functions must be validated on independent test data:

\begin{enumerate}[itemsep=2pt]
\item Split SMA-seq data into calibration set (fit recalibration function) and test set (evaluate)
\item Apply recalibration function to test set predicted scores
\item Recompute $d$ and ECE on calibrated scores
\item Verify improvement: calibrated ECE should be < 0.05 and $|d| < 0.10$
\item If criteria not met, try alternative recalibration method or expand calibration data
\end{enumerate}

\textbf{Critical warning:} Never calibrate and validate on the same data. This produces artificially optimistic calibration assessment and may yield models that overfit to calibration set.

\subsection{Integration with Haplotype Classification}

SMA-seq measurements directly enhance the likelihood calculations and posterior inference procedures developed in Chapters~\ref{chap:signal-to-sequence} and \ref{chap:haplotype-posteriors}. Rather than relying on basecaller-reported quality scores that may be systematically miscalibrated, we can incorporate empirically-measured error rates into per-read likelihood computations.

The workflow weight calculation in \CEref{11} can be enhanced with sequence-specific empirical error rates from SMA-seq. For each possible source fragment $s$ from haplotype $h_i$, we compute:

\begin{equation}
\pi_i(s) = P(s|h_i) \cdot f_{\text{emp}}(\ell(s)) \cdot A(\ell(s)) \cdot [1 - \text{EER}(s)]
\label{eq_13_10}
\end{equation}

where $\text{EER}(s)$ comes from SMA-seq measurements on sequences similar to $s$ (Experimental Error Rate from Chapter~\ref{chap:sma-seq}). When direct SMA-seq measurements are unavailable for a specific sequence, we can interpolate based on sequence similarity or k-mer composition.

This empirical error incorporation provides several benefits. First, it grounds likelihood calculations in measured performance rather than theoretical assumptions. Second, it automatically accounts for technology-specific biases and context dependencies. Third, it enables principled comparison across sequencing platforms by using platform-specific SMA-seq calibrations.

\section{Generalization Testing}
\label{sec:generalization-testing}

Fine-tuned models must generalize beyond the specific standards used for training. This section describes validation protocols ensuring robust performance across diverse conditions.

\subsection{Cross-Platform Validation}

If deploying across multiple flowcell types or sequencing platforms, validate on each:

\textbf{Protocol:}
\begin{enumerate}[itemsep=2pt]
\item Fine-tune model using Platform A data (e.g., ONT R9.4.1)
\item Sequence identical standards on Platform B (e.g., ONT R10.4.1)
\item Evaluate fine-tuned model on Platform B data without additional training
\item Compare performance: acceptable generalization if Platform B accuracy $\geq$ 95\% of Platform A accuracy
\item If generalization fails, include Platform B data in fine-tuning (multi-platform training)
\end{enumerate}

\subsection{Temporal Validation}

Sequencing performance drifts over time due to flowcell aging, reagent lot variation, and environmental factors. Validate temporal stability:

\textbf{Protocol:}
\begin{enumerate}[itemsep=2pt]
\item Fine-tune model using data from Month 1
\item Sequence validation standards monthly for 6-12 months
\item Track accuracy trends: acceptable if no significant decline over time
\item If performance degrades, identify root cause (reagent change, instrument drift)
\item Re-fine-tune if necessary or implement periodic recalibration
\end{enumerate}

\subsection{Sequence Diversity Testing}

Fine-tuning on specific pharmacogenes should not degrade performance on unrelated sequences:

\textbf{Protocol:}
\begin{enumerate}[itemsep=2pt]
\item Prepare control standards from non-target genes (e.g., housekeeping genes)
\item Sequence controls before and after fine-tuning
\item Compare accuracies: acceptable if post-fine-tuning accuracy $\geq$ 98\% of pre-fine-tuning
\item If degradation occurs, adjust regularization or reduce learning rate
\item Consider multi-task learning approaches that jointly optimize target and general performance
\end{enumerate}

\subsection{Batch Effect Robustness}

DNA preparation, library construction, and sequencing introduce batch-specific variation. Validate robustness:

\textbf{Protocol:}
\begin{enumerate}[itemsep=2pt]
\item Prepare multiple independent batches of same standard
\item Use different operators, reagent lots, and flowcells
\item Sequence all batches, evaluate fine-tuned model
\item Compute coefficient of variation (CV) in accuracy across batches
\item Acceptable if CV $< 5\%$; investigate if CV $> 10\%$
\end{enumerate}

\section{Quality Control Gates}
\label{sec:basecaller-qc}

SMA-seq metrics also enhance the quality control framework detailed in Chapter~\ref{chap:qc-gates}. Specifically, the quality overestimation threshold in \CEref{14} now has empirical grounding:

\begin{proposition}[SMA-seq-Based Quality Gate]
For a sequencing run to pass quality control, the quality overestimation fraction measured on control standards must satisfy:
\begin{equation}
d \leq 0.30 \text{ with 95\% confidence}
\label{eq_13_11}
\end{equation}
Runs exceeding this threshold require investigation for systematic miscalibration before clinical reporting.
\end{proposition}

Additional QC gates specific to fine-tuned basecallers:

\begin{table}[!htbp]
\centering
\caption{Basecaller Fine-Tuning QC Gates}
\label{tab:basecaller-qc}
\begin{tabular}{p{4cm}p{4cm}p{4.5cm}}
\toprule
\textbf{Metric} & \textbf{Threshold} & \textbf{Action if Failed} \\
\midrule
Test set accuracy & $\geq$ 95\% of pre-training & Re-optimize hyperparameters \\
Validation-test gap & $< 0.05$ accuracy difference & Increase regularization \\
Calibration error (ECE) & $< 0.05$ & Apply recalibration (Section~\ref{sec:quality-calibration}) \\
Cross-platform accuracy & $\geq$ 95\% of source platform & Include target platform in training \\
Temporal stability (6 mo) & No significant trend & Implement periodic monitoring \\
Batch CV & $< 0.05$ & Investigate batch-specific factors \\
\bottomrule
\end{tabular}
\end{table}

\section{Practical Example: CYP2D6 Fine-Tuning}
\label{sec:cyp2d6-example}

We conclude with a worked example demonstrating the complete fine-tuning workflow for CYP2D6 pharmacogenomics applications.

\subsection{Problem Statement}

CYP2D6 is a highly polymorphic pharmacogene with $>$ 100 star alleles, complex structural variants (gene deletions, duplications, hybrids), and clinical importance for drug metabolism prediction. Standard ONT basecallers achieve 92\% accuracy on CYP2D6 alleles, below the 95\% threshold required for clinical applications. Goal: fine-tune basecaller to achieve $\geq$ 95\% accuracy.

\subsection{Standard Construction}

Following Chapter~\ref{chap:plasmid-standards}, we synthesized 25 plasmid standards:
\begin{itemize}[itemsep=1pt]
\item 15 common star alleles (*1, *2, *3, *4, *5, *6, *9, *10, *17, *29, *41, etc.)
\item 5 rare clinical alleles (*11, *14, *36, *68, *83)
\item 5 hybrid and structural variant alleles
\end{itemize}

Each standard verified via Sanger sequencing, prepared at > 95\% purity per Chapter~\ref{chap:purity}.

\subsection{Initial SMA-seq Assessment}

Sequenced all standards on ONT R10.4.1 (200$\times$ coverage each), performed SMA-seq analysis (Chapter~\ref{chap:sma-seq}):

\textbf{Results:}
\begin{itemize}[itemsep=1pt]
\item Overall TPR: 0.92 (below clinical threshold)
\item Quality overestimation $d = 0.41$ (exceeds 0.30 threshold)
\item Problematic alleles: *5 (deletion), *36 (long homopolymer), *68 (GC-rich exon)
\item Calibration error ECE = 0.08 (marginal)
\end{itemize}

These metrics justified fine-tuning effort.

\subsection{Fine-Tuning Execution}

\textbf{Data preparation:}
\begin{itemize}[itemsep=1pt]
\item Extracted 35,000 reads (signals + sequences) from SMA-seq data
\item Train/val/test split: 70/15/15\% stratified by allele
\item Augmented training data via signal perturbation and reverse complement
\end{itemize}

\textbf{Fine-tuning procedure:}
\begin{itemize}[itemsep=1pt]
\item Pre-trained model: Dorado v0.4.0 HAC
\item Fine-tuned: top 3 layers of RNN stack + output layer
\item Loss: CTC loss (Equation~\ref{eq_13_4})
\item Hyperparameters: $\eta = 10^{-4}$, batch size 128, $\lambda = 10^{-4}$, dropout $p=0.3$
\item Training: 20 epochs with early stopping (patience = 5)
\item Computation: 8 hours on NVIDIA A100 GPU
\end{itemize}

\subsection{Post-Fine-Tuning Assessment}

Re-sequenced standards with fine-tuned model:

\textbf{Sequence accuracy:}
\begin{itemize}[itemsep=1pt]
\item Overall TPR improved: 0.92 $\to$ 0.96 (\textbf{+4\% absolute, meets clinical threshold})
\item Problematic alleles improved: *5 (0.85 $\to$ 0.94), *36 (0.88 $\to$ 0.95), *68 (0.90 $\to$ 0.96)
\item No degradation on control genes (housekeeping genes maintained $\geq$ 98\% accuracy)
\end{itemize}

\textbf{Quality score calibration:}
\begin{itemize}[itemsep=1pt]
\item Quality overestimation improved: $d = 0.41 \to 0.18$ (\textbf{now passes 0.30 threshold})
\item Applied isotonic recalibration to further improve: $d = 0.18 \to 0.08$
\item Calibration error reduced: ECE = 0.08 $\to$ 0.03 (\textbf{well-calibrated})
\end{itemize}

\textbf{Generalization validation:}
\begin{itemize}[itemsep=1pt]
\item Cross-flowcell: validated on 5 different R10.4.1 flowcells, accuracy CV = 3\%
\item Temporal: monitored over 6 months, no significant accuracy decline
\item Cross-platform: acceptable performance on R9.4.1 (94\% of R10.4.1 accuracy)
\end{itemize}

\textbf{Conclusion:} Fine-tuning successfully improved CYP2D6 star allele classification to clinical standards while maintaining calibration and generalization properties.

\section{Variable Summary and Reference}
\label{sec:variable-summary-basecaller}

This section provides a comprehensive summary of all key variables used in this chapter for basecaller fine-tuning and calibration. These variables form the core notation for neural network optimization and quality assessment throughout the framework.

\subsection{Variable Summary Table}

\begin{vartable}
\varrow{$\boldsymbol{\theta}$}{Neural network model parameters (weights and biases).}
       {dimensionless parameter vector}
       {Optimized via gradient descent on training data; initialized from pre-trained model.}

\varrow{$\eta$}{Learning rate for gradient descent optimization.}
       {dimensionless step size}
       {Set via hyperparameter search; typical range $10^{-5}$ to $10^{-3}$ for fine-tuning.}

\varrow{$\alpha$}{Learning rate reduction factor for fine-tuning.}
       {dimensionless scaling factor}
       {Chosen in range $[0.01, 0.1]$ to prevent catastrophic forgetting.}

\varrow{$\mathcal{L}_{\text{CE}}$}{Cross-entropy loss for sequence prediction.}
       {nats or bits (log-probability)}
       {Computed from predicted vs. ground truth sequences during training.}

\varrow{$\mathcal{L}_{\text{CTC}}$}{Connectionist Temporal Classification loss.}
       {nats or bits (log-probability)}
       {Computed by marginalizing over all valid alignments for variable-length sequences.}

\varrow{$\text{ECE}$}{Expected Calibration Error: measure of quality score calibration.}
       {dimensionless (0--1)}
       {Computed from binned empirical accuracies vs. predicted qualities; ECE $< 0.05$ indicates good calibration.}

\varrow{$d$}{Quality overestimation fraction from SMA-seq.}
       {fraction (0--1)}
       {Fraction of reads where predicted quality exceeds empirical quality; threshold $d \leq 0.30$ for clinical use.}

\varrow{$Q_{\text{pred}}$}{Mean predicted Phred quality score for a read.}
       {Phred units}
       {Reported by basecaller; averaged over all bases in read.}

\varrow{$Q_{\text{emp}}$}{Empirical Phred quality from alignment to known standard.}
       {Phred units}
       {Computed from observed error rate in SMA-seq experiments.}

\varrow{$\lambda$}{Regularization strength parameter.}
       {dimensionless coefficient}
       {Typical range $10^{-5}$ to $10^{-3}$; controls weight decay penalty.}

\varrow{$\mathcal{D}_{\text{SEER}}$}{Training dataset of (signal, sequence) pairs from SMA-seq.}
       {set of paired examples}
       {Generated from sequencing physical standards with known sequences.}

\varrow{$\mathcal{R}(\boldsymbol{\theta})$}{Regularization penalty (typically $L_2$ norm).}
       {dimensionless}
       {Prevents overfitting by penalizing large parameter values.}
\end{vartable}

\subsection{Detailed Variable Reference Boxes}

This section provides in-depth reference information for the most important variables in basecaller fine-tuning, including physical descriptions, units, measurement methods, and concrete examples.

\begin{varbox}{$\boldsymbol{\theta}$}
\textbf{Physical description.}
The complete set of trainable parameters in the neural network basecaller model. For modern basecallers, this includes millions to hundreds of millions of weights and biases across convolutional layers, recurrent (LSTM/GRU) layers, and output classification layers.

\textbf{Units.}
Dimensionless parameter vector; length depends on model architecture (typically $10^6$ to $10^8$ parameters).

\textbf{Measurement / determination.}
Initially loaded from pre-trained model checkpoint. During fine-tuning, parameters are updated via stochastic gradient descent (SGD) or adaptive optimizers (Adam, RMSprop) to minimize the loss function on training data.

\textbf{Example.}
For ONT Guppy/Dorado basecallers, the parameter vector $\boldsymbol{\theta}$ contains approximately 20-40 million weights distributed across 5-8 convolutional layers, 3-5 bidirectional LSTM layers, and a final CTC/softmax output layer. Fine-tuning typically updates only the top 2-3 layers (5-10 million parameters) while freezing early feature extraction layers.
\end{varbox}

\begin{varbox}{$\eta$, $\alpha$}
\textbf{Physical description.}
$\eta$ is the learning rate controlling the step size for gradient descent updates. $\alpha$ is the reduction factor that scales the pre-training learning rate down for fine-tuning to prevent catastrophic forgetting.

\textbf{Units.}
Both are dimensionless scaling factors.

\textbf{Measurement / determination.}
$\eta_{\text{pretrain}}$ is specified by the original model developers. $\alpha$ is chosen via grid search over validation performance, typically in range $[0.01, 0.1]$. The fine-tuning learning rate is $\eta_{\text{fine-tune}} = \alpha \cdot \eta_{\text{pretrain}}$.

\textbf{Example.}
If the pre-trained Dorado model used $\eta_{\text{pretrain}} = 10^{-3}$, fine-tuning with $\alpha = 0.1$ gives $\eta_{\text{fine-tune}} = 10^{-4}$. For a small dataset of 1,000 training examples, reducing to $\alpha = 0.01$ ($\eta = 10^{-5}$) may be necessary to prevent overfitting.
\end{varbox}

\begin{varbox}{$\mathcal{L}_{\text{CE}}$}
\textbf{Physical description.}
Cross-entropy loss measuring the negative log-likelihood of the ground truth sequence under the model's predicted probability distribution. Standard loss function for sequence classification tasks.

\textbf{Units.}
Log-probability (nats if natural log, bits if $\log_2$); typically computed as nats for compatibility with PyTorch/TensorFlow.

\textbf{Measurement / determination.}
Computed during training by comparing the basecaller's predicted base probabilities $P(\hat{s}_i = b)$ against the known ground truth sequence $s$ from SMA-seq standards. Lower values indicate better model fit.

\textbf{Example.}
For a perfect prediction where the model assigns $P(\hat{s}_i = s_i) = 0.999$ at all $L = 5{,}000$ positions, the cross-entropy loss is approximately:
\[
\mathcal{L}_{\text{CE}} \approx -5000 \times \log(0.999) \approx 5.0 \text{ nats}.
\]
For a poor model averaging $P(\hat{s}_i = s_i) = 0.5$, the loss would be $-5000 \times \log(0.5) \approx 3466$ nats.
\end{varbox}

\begin{varbox}{$\mathcal{L}_{\text{CTC}}$}
\textbf{Physical description.}
Connectionist Temporal Classification loss for models that must align variable-length input signals to variable-length output sequences. Marginalizes over all possible alignments between signal and sequence.

\textbf{Units.}
Log-probability (nats or bits).

\textbf{Measurement / determination.}
Computed via dynamic programming (forward-backward algorithm) over the lattice of valid alignments. The CTC loss sums over all paths through the alignment space that decode to the target sequence.

\textbf{Example.}
For a nanopore read with 40,000 signal samples mapping to a 7,000 bp sequence, there are exponentially many valid alignments (due to variable dwell times per base). CTC efficiently computes:
\[
\mathcal{L}_{\text{CTC}} = -\log \sum_{\pi \in \mathcal{A}(s)} P(\pi | x)
\]
where $\mathcal{A}(s)$ contains all alignments consistent with the target sequence. Typical values range from 100-10,000 nats depending on sequence length and model confidence.
\end{varbox}

\begin{varbox}{$\text{ECE}$}
\textbf{Physical description.}
Expected Calibration Error quantifies the average discrepancy between predicted quality scores and empirical accuracy across the quality score range. Measures how well basecaller confidence estimates match observed error rates.

\textbf{Units.}
Dimensionless fraction in $[0,1]$; can also be expressed as percentage or Phred units.

\textbf{Measurement / determination.}
Computed by binning reads by predicted quality, measuring empirical accuracy in each bin from SMA-seq alignments, and computing weighted average of absolute differences. Requires large validation set (10,000+ reads) for stable estimates.

\textbf{Example.}
Consider binning quality scores $[Q20, Q25, Q30, Q35]$ with 2,500 reads per bin. If predicted qualities are $[22.5, 27.5, 32.5, 37.5]$ but empirical accuracies correspond to $[Q20, Q25, Q28, Q34]$, the ECE is:
\[
\text{ECE} = \frac{1}{4}\left(|22.5-20| + |27.5-25| + |32.5-28| + |37.5-34|\right) = \frac{1}{4}(2.5 + 2.5 + 4.5 + 3.5) = 3.25.
\]
This ECE $> 0.05$ indicates miscalibration requiring correction.
\end{varbox}

\begin{keytakeaways}
This chapter presented comprehensive strategies for fine-tuning basecaller neural networks using high-quality training data from physical standards:

\textbf{Basecaller Architecture Foundations:}
\begin{itemize}
\item \textbf{Signal-to-Sequence Transformation:} Modern basecallers use deep neural networks to map continuous time-series signals (nanopore current at 4-5 kHz, PacBio fluorescence at ~100 Hz) to discrete nucleotide sequences with quality scores

\item \textbf{ONT Architectures:} 1D convolutions extract local signal features, bidirectional LSTM/GRU layers capture temporal dependencies, CTC decoder handles variable-length alignment, softmax output over extended alphabet (A, C, G, T, blank)

\item \textbf{Platform-Specific Considerations:} R10.4.1's 9-mer context requires more diverse training data than R9.4.1's 5-mer context; PacBio focuses on GC-rich regions and homopolymer accuracy

\item \textbf{Pre-Trained Models:} Always start with latest vendor models (ONT Guppy/Dorado, PacBio SMRT Link) rather than training from scratch (thousands of GPU-hours, millions of examples)
\end{itemize}

\textbf{Fine-Tuning vs. Training from Scratch:}
\begin{itemize}
\item \textbf{Transfer Learning Advantage:} Fine-tuning requires 1,000-10,000 examples and hours-to-days of computation vs. millions of examples and weeks for de novo training

\item \textbf{When to Fine-Tune:} Targeting specific sequence contexts (pharmacogenes, homopolymers), adapting to new chemistry/pore, correcting systematic biases, improving quality calibration

\item \textbf{When to Train from Scratch:} Fundamentally new signal modality, no pre-trained model available, or vendor model unusable (rare)
\end{itemize}

\textbf{Transfer Learning Protocol:}
\begin{itemize}
\item \textbf{Layer Selection:} Freeze early convolutional layers (general signal features), fine-tune middle recurrent layers (sequence context), retrain final classification layers (sequence-specific patterns)

\item \textbf{Hyperparameter Selection:} Lower learning rate ($10^{-4}$ to $10^{-5}$ vs. $10^{-3}$ for scratch), smaller batch sizes (16-64), shorter training (10-50 epochs with early stopping)

\item \textbf{Active Learning:} Prioritize training examples showing high current error rates (TPR $< 0.90$), challenging motifs (homopolymers, GC extremes), underrepresented contexts ($k$-mer frequency $< 10^{-4}$)

\item \textbf{Regularization:} Dropout $p \in [0.1, 0.3]$, $L_2$ penalty $\lambda \sim 10^{-4}$, early stopping on validation set from held-out standards
\end{itemize}

\textbf{Quality Score Calibration:}
\begin{itemize}
\item \textbf{Calibration Assessment:} Compare empirical error rate $\epsilon_{\text{emp}}$ from SMA-seq against predicted from quality: $\epsilon_{\text{pred}} = 10^{-Q/10}$; compute calibration gap $\Delta Q = Q_{\text{emp}} - Q_{\text{pred}}$

\item \textbf{Isotonic Regression Calibration:} Fit monotonic mapping from predicted quality to empirical quality, preserving rank order while correcting systematic bias

\item \textbf{Acceptance Criteria:} Calibration gap $|\Delta Q| < 2$ Phred units across Q20-Q40 range for clinical deployment

\item \textbf{Platform Differences:} ONT basecallers often overestimate quality ($\Delta Q > 0$), PacBio HiFi reads generally well-calibrated ($\Delta Q \approx 0$)
\end{itemize}

\textbf{Validation Requirements:}
\begin{itemize}
\item \textbf{Cross-Flowcell Generalization:} Validate on $\geq 3$ different flowcells, accuracy CV $< 5\%$ acceptable

\item \textbf{Temporal Stability:} Monitor performance over $\geq 3$ months, no significant accuracy decline ($p > 0.05$ by trend test)

\item \textbf{Cross-Platform Robustness:} Acceptable performance on related platforms (e.g., R10.4.1 fine-tuned model at $\geq 90\%$ of performance on R9.4.1)

\item \textbf{SMA-seq Validation:} All fine-tuned models must pass SMA-seq QC gates before clinical deployment (Chapter~\ref{chap:qc-gates})
\end{itemize}

\textbf{Key Principles:}
\begin{itemize}
\item Leverage pre-trained models via transfer learning rather than training from scratch
\item Use signal databases from SMA-seq experiments (Chapter~\ref{chap:noisy_labels}) for ground truth labels
\item Select training examples strategically via active learning to target performance gaps
\item Apply appropriate regularization and early stopping to prevent overfitting
\item Assess and correct quality score calibration using empirical SMA-seq measurements
\item Validate generalization across platforms, batches, and temporal conditions
\item Integrate fine-tuned models with SMA-seq QC gates (Chapter~\ref{chap:qc-gates})
\end{itemize}

\textbf{Iterative Improvement Loop:} Fine-tuning closes the loop established in Chapters~\ref{chap:sma-seq} and \ref{chap:noisy_labels}: empirical error measurements identify performance gaps → signal databases provide training data → fine-tuning improves models → SMA-seq validation confirms improvement. This iterative cycle enables continuous model improvement as new challenging sequence contexts are identified and targeted.

\textbf{Framework Integration:} Fine-tuned basecallers with calibrated quality scores directly feed into the Bayesian haplotype classification machinery (Chapters~\ref{chap:signal-to-sequence} through \ref{chap:haplotype-posteriors}). Empirical error rates from SMA-seq enhance likelihood calculations, while calibrated quality scores ensure proper uncertainty quantification in posterior distributions. The complete pipeline—from physical standards through fine-tuning to validated haplotype classification—provides a rigorous, empirically-grounded approach to clinical genomics.

\textbf{Related Topics:} See Chapter~\ref{chap:sma-seq} for empirical error rate measurement methodology, Chapter~\ref{chap:noisy_labels} for signal database construction and robust training strategies, Chapter~\ref{chap:qc-gates} for quality control gates incorporating basecaller performance metrics, and Chapter~\ref{chap:cyp2d6} for extended clinical validation of the CYP2D6 fine-tuning example.
\end{keytakeaways}
