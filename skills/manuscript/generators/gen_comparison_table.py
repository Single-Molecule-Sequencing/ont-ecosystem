#!/usr/bin/env python3
"""
Generate multi-experiment comparison table.

Creates a formatted table comparing metrics across multiple experiments.

Usage:
    gen_comparison_table.py <exp1> <exp2> ... --output <path> --format <tex|csv|json|html>
"""

import argparse
import json
import sys
from pathlib import Path

# Add parent directories to path for imports
sys.path.insert(0, str(Path(__file__).parent.parent.parent.parent / "bin"))

try:
    from ont_context import load_experiment_context
    HAS_CONTEXT = True
except ImportError:
    HAS_CONTEXT = False


def format_number(value, precision=1):
    """Format number with commas and optional precision"""
    if value is None:
        return "N/A"
    if isinstance(value, float):
        return f"{value:,.{precision}f}"
    if isinstance(value, int):
        return f"{value:,}"
    return str(value)


def generate_comparison_table(contexts, output_path: Path, format: str = "tex",
                              metrics: list = None):
    """
    Generate comparison table from multiple experiment contexts.

    Args:
        contexts: List of ExperimentContext objects
        output_path: Output file path
        format: Output format (tex, csv, json, html)
        metrics: List of metrics to include (None = all)

    Returns:
        Path to generated file
    """
    if not contexts:
        print("Error: No experiments to compare")
        return None

    # Define available metrics
    all_metrics = [
        ("Total Reads", lambda c: c.statistics.total_reads if c.statistics else None),
        ("Total Bases", lambda c: c.statistics.total_bases if c.statistics else None),
        ("Mean Q-Score", lambda c: c.statistics.mean_qscore if c.statistics else None),
        ("N50", lambda c: c.statistics.n50 if c.statistics else None),
        ("Signal Positive %", lambda c: c.end_reasons.signal_positive_pct if c.end_reasons else None),
        ("Unblock %", lambda c: c.end_reasons.unblock_pct if c.end_reasons else None),
        ("Quality Grade", lambda c: c.quality_grade),
    ]

    # Filter metrics if specified
    if metrics:
        all_metrics = [(name, func) for name, func in all_metrics if name in metrics]

    # Build table data
    headers = ["Metric"] + [c.name[:20] for c in contexts]
    rows = []

    for metric_name, metric_func in all_metrics:
        row = [metric_name]
        for ctx in contexts:
            value = metric_func(ctx)
            row.append(format_number(value))
        rows.append(row)

    # Write output
    output_path = Path(output_path)

    if format == "json":
        data = {
            "experiments": [c.id for c in contexts],
            "metrics": {}
        }
        for row in rows:
            metric_name = row[0]
            data["metrics"][metric_name] = {
                contexts[i].id: row[i+1] for i in range(len(contexts))
            }
        with open(output_path, "w") as f:
            json.dump(data, f, indent=2)

    elif format == "csv":
        with open(output_path, "w") as f:
            f.write(",".join(headers) + "\n")
            for row in rows:
                f.write(",".join(str(v).replace(",", "") for v in row) + "\n")

    elif format == "tex":
        with open(output_path, "w") as f:
            f.write("% Experiment Comparison Table\n")
            f.write("% Generated by ONT Manuscript Studio\n")
            f.write("\\begin{table}[htbp]\n")
            f.write("\\centering\n")
            f.write("\\caption{Experiment Comparison}\n")
            f.write("\\label{tab:experiment_comparison}\n")

            # Column format: l for metric, c for each experiment
            col_format = "l" + "c" * len(contexts)
            f.write(f"\\begin{{tabular}}{{{col_format}}}\n")
            f.write("\\toprule\n")

            # Header row
            header_row = " & ".join([_latex_escape(h) for h in headers])
            f.write(f"\\textbf{{{headers[0]}}} & " +
                   " & ".join([f"\\textbf{{{_latex_escape(h)}}}" for h in headers[1:]]) +
                   " \\\\\n")
            f.write("\\midrule\n")

            for row in rows:
                f.write(" & ".join([_latex_escape(str(v)) for v in row]) + " \\\\\n")

            f.write("\\bottomrule\n")
            f.write("\\end{tabular}\n")
            f.write("\\end{table}\n")

    elif format == "html":
        with open(output_path, "w") as f:
            f.write("<!-- Experiment Comparison Table -->\n")
            f.write("<!-- Generated by ONT Manuscript Studio -->\n")
            f.write('<table class="experiment-comparison">\n')
            f.write("  <caption>Experiment Comparison</caption>\n")
            f.write("  <thead>\n")
            f.write("    <tr>" + "".join([f"<th>{_html_escape(h)}</th>" for h in headers]) + "</tr>\n")
            f.write("  </thead>\n")
            f.write("  <tbody>\n")

            for row in rows:
                f.write("    <tr>" + "".join([f"<td>{_html_escape(str(v))}</td>" for v in row]) + "</tr>\n")

            f.write("  </tbody>\n")
            f.write("</table>\n")

    return output_path


def _latex_escape(text: str) -> str:
    """Escape special LaTeX characters"""
    replacements = {
        '&': r'\&', '%': r'\%', '$': r'\$', '#': r'\#',
        '_': r'\_', '{': r'\{', '}': r'\}',
    }
    for char, replacement in replacements.items():
        text = text.replace(char, replacement)
    return text


def _html_escape(text: str) -> str:
    """Escape HTML special characters"""
    replacements = {'&': '&amp;', '<': '&lt;', '>': '&gt;', '"': '&quot;'}
    for char, replacement in replacements.items():
        text = text.replace(char, replacement)
    return text


def main():
    parser = argparse.ArgumentParser(description="Generate experiment comparison table")
    parser.add_argument("experiment_ids", nargs="+", help="Experiment IDs to compare")
    parser.add_argument("--output", "-o", required=True, help="Output path")
    parser.add_argument("--format", default="tex", choices=["tex", "csv", "json", "html"],
                       help="Output format")
    parser.add_argument("--metrics", nargs="+", help="Specific metrics to include")

    args = parser.parse_args()

    if not HAS_CONTEXT:
        print("Error: ont_context module required")
        sys.exit(1)

    contexts = []
    for exp_id in args.experiment_ids:
        ctx = load_experiment_context(exp_id)
        if ctx is None:
            print(f"Warning: Experiment not found: {exp_id}")
            continue
        contexts.append(ctx)

    if len(contexts) < 2:
        print("Error: Need at least 2 experiments to compare")
        sys.exit(1)

    output_path = Path(args.output)
    result = generate_comparison_table(
        contexts, output_path, args.format,
        metrics=args.metrics
    )

    if result:
        print(f"Generated: {result}")
    else:
        sys.exit(1)


if __name__ == "__main__":
    main()
