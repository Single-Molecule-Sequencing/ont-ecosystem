name: ONT Analysis Tasks

on:
  workflow_dispatch:
    inputs:
      analysis:
        description: 'Analysis type to run'
        required: true
        type: choice
        options:
          - end-reason
          - basecalling-resources
          - alignment-editdist
          - monitor-snapshot
          - experiment-db-stats
          - context-equations
          - skill-validation
      data_source:
        description: 'Data source (for end-reason/alignment)'
        required: false
        type: string
        default: 'test-data'
      output_format:
        description: 'Output format'
        required: false
        type: choice
        default: 'json'
        options:
          - json
          - csv
          - text
      include_plot:
        description: 'Generate visualization plots'
        required: false
        type: boolean
        default: true

env:
  PYTHONPATH: ${{ github.workspace }}/bin:${{ github.workspace }}

jobs:
  analyze:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v4
        with:
          lfs: true

      - name: Set up Python
        uses: actions/setup-python@v5
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install pyyaml jsonschema numpy pandas matplotlib
          pip install pysam edlib h5py || true
          pip install pod5 || true

      - name: Create test data directory
        run: mkdir -p test-data outputs

      # End Reason Analysis
      - name: End Reason Analysis
        if: inputs.analysis == 'end-reason'
        run: |
          echo "## End Reason Analysis" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          # Generate synthetic test data if needed
          python -c "
          import json
          from pathlib import Path

          # Create mock sequencing summary for testing
          mock_data = {
              'test_mode': True,
              'summary': {
                  'total_reads': 10000,
                  'signal_positive': 7500,
                  'signal_negative': 1500,
                  'unblock_mux_change': 500,
                  'data_service_unblock_mux_change': 300,
                  'other': 200
              },
              'quality_status': 'PASS',
              'signal_positive_pct': 75.0
          }

          Path('test-data/mock_summary.json').write_text(json.dumps(mock_data, indent=2))
          print('Created mock test data')
          "

          echo "### Test Results" >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`json" >> $GITHUB_STEP_SUMMARY
          cat test-data/mock_summary.json >> $GITHUB_STEP_SUMMARY
          echo "\`\`\`" >> $GITHUB_STEP_SUMMARY

          # Run end reason script in validation mode
          python bin/end_reason.py --help
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "End reason analysis script validated successfully." >> $GITHUB_STEP_SUMMARY

      # Basecalling Resource Calculator
      - name: Calculate Basecalling Resources
        if: inputs.analysis == 'basecalling-resources'
        run: |
          echo "## Basecalling Resource Calculator" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python -c "
          import sys
          sys.path.insert(0, 'bin')

          # Test resource calculation logic
          models = ['fast', 'hac', 'sup']
          data_sizes_gb = [1, 10, 50, 100]

          print('| Model | Data Size | Est. GPU Hours | Memory (GB) |')
          print('|-------|-----------|----------------|-------------|')

          for model in models:
              for size in data_sizes_gb:
                  # Simplified resource estimation
                  if model == 'fast':
                      gpu_hours = size * 0.1
                      memory = 8
                  elif model == 'hac':
                      gpu_hours = size * 0.5
                      memory = 16
                  else:  # sup
                      gpu_hours = size * 2.0
                      memory = 32

                  print(f'| {model} | {size} GB | {gpu_hours:.1f} | {memory} |')
          " >> $GITHUB_STEP_SUMMARY

      # Alignment Edit Distance
      - name: Alignment Edit Distance Demo
        if: inputs.analysis == 'alignment-editdist'
        run: |
          echo "## Edit Distance Computation" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python -c "
          try:
              import edlib

              # Demo sequences
              tests = [
                  ('ACGTACGT', 'ACGTACGT', 'Identical'),
                  ('ACGTACGT', 'ACGTTCGT', 'Single substitution'),
                  ('ACGTACGT', 'ACGTACGTA', 'Single insertion'),
                  ('ACGTACGT', 'ACGTCGT', 'Single deletion'),
                  ('GATTACA', 'GCATGCU', 'Multiple edits'),
              ]

              print('| Seq1 | Seq2 | Type | Edit Distance | Normalized |')
              print('|------|------|------|---------------|------------|')

              for seq1, seq2, desc in tests:
                  result = edlib.align(seq1, seq2, task='distance')
                  dist = result['editDistance']
                  norm = dist / max(len(seq1), len(seq2))
                  print(f'| {seq1} | {seq2} | {desc} | {dist} | {norm:.3f} |')

          except ImportError:
              print('edlib not available - showing expected output format')
              print('')
              print('| Seq1 | Seq2 | Type | Edit Distance | Normalized |')
              print('|------|------|------|---------------|------------|')
              print('| ACGTACGT | ACGTACGT | Identical | 0 | 0.000 |')
          " >> $GITHUB_STEP_SUMMARY

      # Monitor Snapshot
      - name: Monitor Snapshot Demo
        if: inputs.analysis == 'monitor-snapshot'
        run: |
          echo "## Sequencing Monitor Metrics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python -c "
          import json
          from datetime import datetime

          # Demo monitoring metrics
          metrics = {
              'snapshot_time': datetime.now().isoformat(),
              'run_status': 'demo',
              'read_stats': {
                  'total_reads': 5000000,
                  'total_bases': 25000000000,
                  'mean_qscore': 15.2,
                  'median_qscore': 16.1,
                  'n50': 12500,
                  'longest_read': 150000
              },
              'throughput': {
                  'reads_per_hour': 50000,
                  'bases_per_hour_gb': 2.5,
                  'estimated_total_gb': 60
              },
              'pore_activity': {
                  'active_pores': 2500,
                  'total_pores': 3000,
                  'active_pct': 83.3
              },
              'quality_alerts': []
          }

          print('### Run Metrics')
          print('')
          print('| Metric | Value |')
          print('|--------|-------|')
          print(f'| Total Reads | {metrics[\"read_stats\"][\"total_reads\"]:,} |')
          print(f'| Total Bases | {metrics[\"read_stats\"][\"total_bases\"]/1e9:.1f} Gb |')
          print(f'| Mean Q-score | {metrics[\"read_stats\"][\"mean_qscore\"]:.1f} |')
          print(f'| N50 | {metrics[\"read_stats\"][\"n50\"]:,} bp |')
          print(f'| Active Pores | {metrics[\"pore_activity\"][\"active_pct\"]:.1f}% |')
          print('')
          print('### Full JSON Output')
          print('\`\`\`json')
          print(json.dumps(metrics, indent=2))
          print('\`\`\`')
          " >> $GITHUB_STEP_SUMMARY

      # Experiment Database Stats
      - name: Experiment Database Stats
        if: inputs.analysis == 'experiment-db-stats'
        run: |
          echo "## Experiment Database Statistics" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python -c "
          import json
          from pathlib import Path

          # Load registry
          registry_path = Path('data/experiment_registry.json')
          if registry_path.exists():
              reg = json.load(open(registry_path))

              print('### Registry Summary')
              print('')
              print('| Metric | Value |')
              print('|--------|-------|')
              print(f'| Total Experiments | {reg.get(\"total_experiments\", 0)} |')
              print(f'| Total Reads | {reg.get(\"total_reads\", 0):,} |')
              print(f'| Total Bases | {reg.get(\"total_bases\", 0)/1e12:.2f} Tb |')
              print(f'| Sample Types | {len(reg.get(\"sample_types\", []))} |')
              print(f'| Flowcell Types | {len(reg.get(\"flowcell_types\", []))} |')
              print('')

              # Category breakdown
              print('### By Category')
              print('')
              for cat, exps in reg.get('by_category', {}).items():
                  print(f'- **{cat}**: {len(exps)} experiments')
          else:
              print('Registry not found')
          " >> $GITHUB_STEP_SUMMARY

      # Context & Equations
      - name: Context & Equations
        if: inputs.analysis == 'context-equations'
        run: |
          echo "## Equations Database" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python -c "
          import sys
          sys.path.insert(0, 'bin')

          try:
              from ont_context import load_equations

              eqs = load_equations()
              equations = eqs.get('equations', {})

              total = len(equations)
              computable = sum(1 for e, d in equations.items()
                              if isinstance(d, dict) and d.get('python'))

              print('### Summary')
              print('')
              print(f'- Total equations: {total}')
              print(f'- Computable (with Python): {computable}')
              print(f'- Documentation only: {total - computable}')
              print('')

              # By chapter/stage
              stages = {}
              for eq_id, eq_data in equations.items():
                  if isinstance(eq_data, dict):
                      stage = eq_data.get('stage', 'unknown')
                      stages[stage] = stages.get(stage, 0) + 1

              print('### By Stage')
              print('')
              print('| Stage | Count |')
              print('|-------|-------|')
              for stage, count in sorted(stages.items()):
                  print(f'| {stage} | {count} |')

          except Exception as e:
              print(f'Error loading equations: {e}')
          " >> $GITHUB_STEP_SUMMARY

      # Skill Validation
      - name: Validate Skills
        if: inputs.analysis == 'skill-validation'
        run: |
          echo "## Skill Validation Report" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY

          python -c "
          import yaml
          import re
          from pathlib import Path

          skills_dir = Path('skills')
          claude_skills_dir = Path('.claude/skills')

          print('### Source Skills (skills/)')
          print('')
          print('| Skill | SKILL.md | Scripts | Assets |')
          print('|-------|----------|---------|--------|')

          for skill_dir in sorted(skills_dir.iterdir()):
              if not skill_dir.is_dir():
                  continue

              skill_md = skill_dir / 'SKILL.md'
              has_skill_md = '✅' if skill_md.exists() else '❌'

              scripts_dir = skill_dir / 'scripts'
              script_count = len(list(scripts_dir.glob('*.py'))) if scripts_dir.exists() else 0

              assets_dir = skill_dir / 'assets'
              asset_count = len(list(assets_dir.iterdir())) if assets_dir.exists() else 0

              print(f'| {skill_dir.name} | {has_skill_md} | {script_count} | {asset_count} |')

          print('')
          print('### Claude Code Skills (.claude/skills/)')
          print('')

          if claude_skills_dir.exists():
              installed = list(claude_skills_dir.iterdir())
              print(f'Installed: {len(installed)} skills')
              print('')
              for skill in sorted(installed):
                  if skill.is_dir():
                      skill_md = skill / 'SKILL.md'
                      status = '✅' if skill_md.exists() else '❌'
                      print(f'- {skill.name} {status}')
          else:
              print('Claude skills directory not found')
          " >> $GITHUB_STEP_SUMMARY

      - name: Upload outputs
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: analysis-output-${{ inputs.analysis }}
          path: |
            outputs/
            test-data/
            *.json
            *.png
            *.csv
          if-no-files-found: ignore
